{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqyLdv5Qn-t5"
      },
      "source": [
        "# Comparing TF-IDF and Word2vec Embeddings: Joke Retrieval\n",
        "\n",
        "A basic information retrieval system, exemplifying tf-idf and word2vec.\n",
        "\n",
        "The goal of the project is to retrieve relevant jokes to a query.\n",
        "\n",
        "Data from https://www.countryliving.com/life/a27452412/best-dad-jokes/ \n",
        "\n",
        "1. Import libraries\n",
        "2. Load data\n",
        "3. Preprocessing\n",
        "4. TF-IDF embeddings and their use for retrieval\n",
        "5. Word2Vec embeddings and their use for retrieval\n",
        "6. Comparison\n",
        "7. Exercises\n",
        "8. Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "conda create --name nlp2 python=3.11\n",
        "conda info --env\n",
        "conda activate nlp\n",
        "conda install nltk\n",
        "conda install scikit-learn\n",
        "conda install gensim\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "m-D6ACIDn-ue"
      },
      "outputs": [],
      "source": [
        "import nltk                         # the natural langauage toolkit, open-source NLP\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "import string                                      # for string.punctuation\n",
        "\n",
        "# consine similarity\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# tf-idf\n",
        "# import the two classes from the scikit-learn library that we will use to convert the preprocessed jokes into a matrix of TF-IDF features and to calculate the similarity between the jokes with a query.\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# word2vec\n",
        "import gensim.downloader as api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\petra\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\petra\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\petra\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download external resources\n",
        "nltk.download('stopwords')          # stopwords are common words that carry less meaning than keywords, usually removed from text\n",
        "nltk.download('wordnet')            # wordnet is a lexical database of English words, used for text analysis\n",
        "nltk.download('punkt')              # punkt is a pre-trained model that helps tokenize words, used for text analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"I'm afraid for the calendar. Its days are numbered.\"\n",
            "\"My wife said I should do lunges to stay in shape. That would be a big step forward.\"\n",
            "\"Why do fathers take an extra pair of socks when they go golfing?\" \"In case they get a hole in one!\"\n",
            "\"Singing in the shower is fun until you get soap in your mouth. Then it's a soap opera.\"\n",
            "\"What do a tick and the Eiffel Tower have in common?\" \"They're both Paris sites.\"\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Dataset of jokes for testing\n",
        "# Open file and read the content in a list\n",
        "with open('best_dad_jokes.txt', 'r', encoding=\"utf-8\") as filehandle:\n",
        "    jokes = [row[:-1] for row in filehandle.readlines()]\n",
        "   # jokes = [row.rstrip()[1:-1] for row in filehandle.readlines()]\n",
        "\n",
        "\n",
        "\n",
        "# Print the first 5 jokes, one per line\n",
        "for joke in jokes[:5]:\n",
        "    print(joke)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Preprocessing\n",
        "\n",
        "The same preprocessing pipeline will be used for both TF-IDF embeddings and for Word2vec embeddings.\n",
        "\n",
        "Defining the preprocessing pipeline:\n",
        "1. Tokenize\n",
        "2. Transform to lower case\n",
        "3. Remove stop words\n",
        "4. Remove unwanted characters\n",
        "5. Lemmatize or (stem)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "TuDRl73uuVj3"
      },
      "outputs": [],
      "source": [
        "#A class for the preprocessing pipeline which can be reused & adapted for several documents\n",
        "\n",
        "class PreprocessingPipeline:\n",
        "    def __init__(self):\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.lemmatizer = WordNetLemmatizer()    # lemmatization is the process of converting a word to its dictionary form\n",
        "     #   self.stemming = PorterStemmer()           # stemming is the process of reducing a word to its root form\n",
        "        self.punctuation = string.punctuation\n",
        "\n",
        "    \n",
        "    #Converting text into tokens\n",
        "    def tokenize(self, text):\n",
        "        return word_tokenize(text)\n",
        "    \n",
        "    #Converting the tokens to lowercase\n",
        "    def case_fold(self, token):\n",
        "        return token.lower()\n",
        "    \n",
        "    #Removing stop-words\n",
        "    def remove_stop_words(self, token):\n",
        "        if token is not None and token not in self.stop_words:\n",
        "            return token\n",
        "        return None\n",
        "\n",
        "    #Removing unwanted characters\n",
        "    def remove_unwanted_characters(self, token):\n",
        "        if token is not None and not token.isalpha():\n",
        "            return None\n",
        "        return token\n",
        "    \n",
        "    #Lemmatizing tokens\n",
        "    def lemmatize(self,token):\n",
        "        lemmatized_token = self.lemmatizer.lemmatize(token)\n",
        "        return lemmatized_token\n",
        "   \n",
        "    def token_stemmer(self,token):\n",
        "        stemmed_token = self.stemming.stem(token)\n",
        "        return stemmed_token\n",
        "\n",
        "    #Preprocessing text by applying all steps from above\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"Returns a list of preprocessed tokens from the input text.\"\"\"\n",
        "        tokens = self.tokenize(text)\n",
        "        preprocessed_tokens = []\n",
        "        for token in tokens:\n",
        "            token = self.case_fold(token)\n",
        "            token = self.remove_stop_words(token)\n",
        "            token = self.remove_unwanted_characters(token)\n",
        "            \n",
        "            if token is not None:\n",
        "                token = self.lemmatize(token)\n",
        "                #token = self.token_stemmer(token)\n",
        "                preprocessed_tokens.append(token)\n",
        "        \n",
        "        return preprocessed_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etSM9rVouc7W",
        "outputId": "2adbf1a5-2d1a-412e-cdcf-d7e2161c011b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"I'm afraid for the calendar. Its days are numbered.\" \n",
            " ---->  ['afraid', 'calendar', 'day', 'numbered']\n",
            "\"My wife said I should do lunges to stay in shape. That would be a big step forward.\" \n",
            " ---->  ['wife', 'said', 'lunge', 'stay', 'shape', 'would', 'big', 'step', 'forward']\n",
            "\"Why do fathers take an extra pair of socks when they go golfing?\" \"In case they get a hole in one!\" \n",
            " ---->  ['father', 'take', 'extra', 'pair', 'sock', 'go', 'golfing', 'case', 'get', 'hole', 'one']\n",
            "\"Singing in the shower is fun until you get soap in your mouth. Then it's a soap opera.\" \n",
            " ---->  ['singing', 'shower', 'fun', 'get', 'soap', 'mouth', 'soap', 'opera']\n",
            "\"What do a tick and the Eiffel Tower have in common?\" \"They're both Paris sites.\" \n",
            " ---->  ['tick', 'eiffel', 'tower', 'common', 'paris', 'site']\n"
          ]
        }
      ],
      "source": [
        "# Preprocess the jokes with the pipeline \n",
        "\n",
        "preprocessor = PreprocessingPipeline()\n",
        "preprocessed_jokes = [preprocessor.preprocess_text(joke) for joke in jokes]\n",
        "\n",
        "\n",
        "# Print the first 5 jokes and their preprocessed tokens\n",
        "for joke, tokens in zip(jokes[:5], preprocessed_jokes[:5]):\n",
        "    print(joke, \"\\n ----> \", tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. TF-IDF embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will first embed the jokes and the query in a TF-IDF space. In this space, each token (word) is a dimension. \n",
        "\n",
        "\n",
        "TF-IDF (Term Frequency-Inverse Document Frequency) evaluates the importance of a word within a document relative to a collection of documents. It consists of two main components: term frequency (TF) and inverse document frequency (IDF). The term frequency measures how often a term appears in a document, while the inverse document frequency quantifies the rarity of a term across the entire document collection. By combining these two factors, TF-IDF assigns higher weights to terms that are frequent within a document but rare across the document collection, thereby emphasizing the significance of terms that are unique to the document.\n",
        "- t: term\n",
        "- d: document\n",
        "- D: collection of documents (corpus)\n",
        "\n",
        "Term frequency\n",
        "$$\n",
        "\\text{TF}(t, d) = \\frac{\\text{Number of times term \\( t \\) appears in document \\( d \\)}}{\\text{Total number of terms in document \\( d \\)}}\\\\\n",
        "$$\n",
        "\n",
        "Inverse document frequency\n",
        "$$\n",
        "\\text{IDF}(t, D) = \\log\\left(\\frac{\\text{Total number of documents in \\( D \\)}}{\\text{Number of documents containing term \\( t \\) in \\( D \\)}}\\right)\\\\\n",
        "$$\n",
        "\n",
        "TF-IDF\n",
        "$$\n",
        "\\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)\\\\\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "We will use the implementation of the TF-IDF vectorizer from the scikit-learn library to convert the preprocessed jokes into a matrix of TF-IDF features.\n",
        "\n",
        "We will then use **cosine similarity** to calculate the similarity between the jokes with a query.\n",
        "\n",
        "$$\n",
        "\\text{Cosine Similarity}(\\mathbf{A}, \\mathbf{B}) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of jokes: 148\n",
            "Embedding dimensions (number of different (non-stop) words): 569\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def dummy_fun(doc):\n",
        "    \"\"\"A dummy function that just returns the input document directly. \n",
        "    This is used to bypass the tokenization and pre-processing steps.\"\"\"\n",
        "    return doc\n",
        "\n",
        "\n",
        "# Create the vectorizer\n",
        "# takes as input a list of lists of strings (the jokes) and returns a matrix of TF-IDF features\n",
        "tfidf_vectorizer =   TfidfVectorizer(     \n",
        "    analyzer='word',\n",
        "    tokenizer=dummy_fun,\n",
        "    preprocessor=dummy_fun,\n",
        "    token_pattern=None)   \n",
        "\n",
        "# Fit the vectorizer on the jokes\n",
        "tfidf_vectorizer.fit(preprocessed_jokes)                          # this can take up to a minute for the large dataset\n",
        "\n",
        "# Transform the jokes\n",
        "tfidf_jokes = tfidf_vectorizer.transform(preprocessed_jokes)\n",
        "\n",
        "# Print the shape of the matrix\n",
        "print(\"Number of jokes:\", tfidf_jokes.shape[0])    \n",
        "print(\"Embedding dimensions (number of different (non-stop) words):\", tfidf_jokes.shape[1])    \n",
        "\n",
        "# the first number is the number of jokes, the second number is the number of unique words in the jokes "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['accidentally' 'account' 'act' 'addicted' 'affect' 'afraid' 'ago' 'agree'\n",
            " 'ahead' 'along' 'alphabet' 'always' 'amazon' 'answer' 'anything'\n",
            " 'apparent' 'apparently' 'apple' 'april' 'area' 'argument' 'arm' 'asked'\n",
            " 'astronaut' 'atom' 'award' 'away' 'baby' 'back' 'backflip' 'bagel'\n",
            " 'banana' 'bank' 'bar' 'bark' 'bartender' 'bathroom' 'bay' 'beach'\n",
            " 'become' 'becomes' 'bee' 'beef' 'beer' 'belt' 'bent' 'best' 'bicycle'\n",
            " 'big' 'billy' 'blockbuster' 'boat' 'body' 'boogie' 'book' 'born' 'bowtie'\n",
            " 'box' 'boxing' 'brace' 'bring' 'brown' 'brush' 'build' 'butter' 'buy'\n",
            " 'ca' 'calendar' 'call' 'called' 'canned' 'capital' 'car' 'card' 'carded'\n",
            " 'case' 'cashier' 'cat' 'catch' 'cent' 'cheese' 'cheeseburger' 'cheesy'\n",
            " 'chemistry' 'chicken' 'child' 'chip' 'chocolate' 'circus' 'classic'\n",
            " 'claus' 'clean' 'climb' 'closed' 'closet' 'clothes' 'cloud' 'clove'\n",
            " 'coffee' 'common' 'company' 'computer' 'concentrate' 'concert'\n",
            " 'construction' 'contest' 'corduroy' 'corn' 'corner' 'cost' 'could'\n",
            " 'count' 'country' 'cow' 'crack' 'crime' 'crossing' 'cut' 'dad' 'dam'\n",
            " 'dance' 'day' 'dear' 'decided' 'deep' 'dentist' 'diet' 'direction'\n",
            " 'disqualified' 'diving' 'doc' 'dog' 'dogwood' 'donor' 'dream' 'drive'\n",
            " 'dry' 'dryer' 'dublin' 'dust' 'dying' 'ear' 'eat' 'eclipse' 'egg'\n",
            " 'eiffel' 'eight' 'elbow' 'elementree' 'elephant' 'elevator' 'end' 'enjoy'\n",
            " 'eraser' 'even' 'ever' 'every' 'everything' 'excited' 'exhausted'\n",
            " 'expensive' 'extra' 'eye' 'facial' 'fact' 'factory' 'fake' 'fanta' 'fast'\n",
            " 'fastest' 'father' 'faux' 'favorite' 'featuring' 'february' 'feel'\n",
            " 'feline' 'fell' 'field' 'filled' 'finger' 'fire' 'fired' 'fish' 'fishing'\n",
            " 'fit' 'fitted' 'flag' 'flew' 'floating' 'flower' 'fly' 'flying' 'fog'\n",
            " 'follow' 'food' 'foot' 'forward' 'fresh' 'fruit' 'fun' 'funny' 'garlic'\n",
            " 'gathering' 'gave' 'get' 'go' 'going' 'golfing' 'good' 'goodbye' 'got'\n",
            " 'grace' 'graveyard' 'grease' 'great' 'grew' 'grow' 'growing' 'guess'\n",
            " 'guilty' 'gut' 'guy' 'hair' 'haircut' 'hand' 'happen' 'happens' 'harry'\n",
            " 'hat' 'hate' 'head' 'headline' 'healthy' 'hear' 'heard' 'heel' 'hill'\n",
            " 'hit' 'hoarse' 'hole' 'honeycomb' 'hot' 'house' 'igloo' 'impasta'\n",
            " 'impossible' 'inappropriate' 'inch' 'insect' 'invented' 'ireland'\n",
            " 'irrelephant' 'island' 'iwitness' 'jam' 'janitor' 'jk' 'joke' 'juice'\n",
            " 'jumped' 'kangaroo' 'kept' 'kick' 'kidnapping' 'kind' 'kleenex' 'know'\n",
            " 'laugh' 'lawyer' 'lazy' 'learn' 'leftover' 'leg' 'lemon' 'let' 'letter'\n",
            " 'lettuce' 'level' 'like' 'limbo' 'line' 'liquor' 'little' 'live' 'long'\n",
            " 'look' 'looked' 'love' 'lunge' 'mad' 'made' 'make' 'making' 'mama' 'man'\n",
            " 'many' 'march' 'mate' 'math' 'matter' 'may' 'meet' 'meltdown' 'milk'\n",
            " 'mind' 'mine' 'minus' 'missed' 'mist' 'monkey' 'moon' 'mop' 'mountain'\n",
            " 'mouth' 'much' 'muffler' 'mugging' 'must' 'na' 'nacho' 'nap' 'neck'\n",
            " 'never' 'neverlands' 'nice' 'nickelback' 'ninja' 'nobody' 'noodle' 'nose'\n",
            " 'nothing' 'notice' 'numbered' 'nut' 'ocean' 'octopus' 'office' 'okay'\n",
            " 'one' 'online' 'opera' 'orange' 'ordered' 'organ' 'outstanding'\n",
            " 'overcrowded' 'p' 'pa' 'packed' 'pair' 'pampered' 'pan' 'paper' 'paris'\n",
            " 'park' 'part' 'pencil' 'penguin' 'people' 'personal' 'peter' 'phone'\n",
            " 'piano' 'pick' 'piggy' 'pilgrim' 'pillow' 'pizza' 'plant' 'play' 'player'\n",
            " 'plus' 'pointless' 'police' 'pony' 'pool' 'poor' 'pop' 'post' 'potato'\n",
            " 'potter' 'pouch' 'pray' 'pretty' 'price' 'prime' 'print' 'prize'\n",
            " 'problem' 'product' 'psychiatrist' 'punch' 'put' 'race' 'reach'\n",
            " 'reaction' 'reading' 'really' 'record' 'refrigerator' 'refuse' 'report'\n",
            " 'resisting' 'rest' 'rhode' 'right' 'robot' 'rowling' 'rumor' 'run' 'sad'\n",
            " 'safe' 'said' 'santa' 'satisfactory' 'say' 'scarecrow' 'school' 'sea'\n",
            " 'seafood' 'seagull' 'see' 'seem' 'seems' 'sell' 'sense' 'serve' 'shady'\n",
            " 'shape' 'share' 'shark' 'shoe' 'shout' 'shower' 'shrinking' 'sick'\n",
            " 'silent' 'since' 'singing' 'site' 'skeleton' 'skin' 'sled' 'sleep'\n",
            " 'sleeping' 'sleigh' 'smelling' 'smith' 'snack' 'sneaker' 'snicker' 'snow'\n",
            " 'snowman' 'soap' 'soccer' 'sock' 'soda' 'sofishticated' 'solve' 'someone'\n",
            " 'something' 'sometimes' 'sore' 'sound' 'space' 'split' 'spoiled' 'sport'\n",
            " 'spread' 'spring' 'sprinter' 'square' 'squirrel' 'st' 'stair' 'stand'\n",
            " 'state' 'stay' 'step' 'stick' 'sticky' 'still' 'store' 'strawberry'\n",
            " 'stream' 'street' 'stuff' 'style' 'suing' 'sundae' 'supply' 'sweet'\n",
            " 'swimming' 'switzerland' 'taco' 'take' 'tantrum' 'teacher' 'tearable'\n",
            " 'tell' 'telling' 'ten' 'tent' 'thing' 'think' 'thought' 'throat' 'throw'\n",
            " 'throwing' 'thunderwear' 'tick' 'tickle' 'time' 'tired' 'tissue'\n",
            " 'toboggan' 'together' 'told' 'tooth' 'total' 'tournament' 'tower'\n",
            " 'traffic' 'trainer' 'tree' 'tried' 'trust' 'try' 'turn' 'two' 'use'\n",
            " 'used' 'using' 'vacation' 'vacuum' 'vegan' 'velcro' 'vet' 'waist' 'walk'\n",
            " 'walking' 'wall' 'wan' 'watch' 'waved' 'way' 'weak' 'wear' 'wearing'\n",
            " 'well' 'wet' 'wheel' 'whenever' 'wife' 'win' 'wise' 'woke' 'work'\n",
            " 'working' 'would' 'wrestle' 'ya' 'year' 'yellow' 'yesterday' 'yolkswagen'\n",
            " 'young' 'zero']\n"
          ]
        }
      ],
      "source": [
        "# Print the words in the vocabulary\n",
        "print(tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# These are the dimensions of our tf-idf embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look at the values "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"I'm afraid for the calendar. Its days are numbered.\"\n",
            "  (0, 333)\t0.5094398629437035\n",
            "  (0, 121)\t0.4705455112225615\n",
            "  (0, 67)\t0.5094398629437035\n",
            "  (0, 5)\t0.5094398629437035\n",
            "['numbered' 'day' 'calendar' 'afraid']\n"
          ]
        }
      ],
      "source": [
        "# Print the first joke and its TF-IDF representation\n",
        "print(jokes[0])\n",
        "print(tfidf_jokes[0])\n",
        "# print the keywords\n",
        "print(tfidf_vectorizer.get_feature_names_out()[tfidf_jokes[0].nonzero()[1]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note, since the dataset is very small (each joke is short and the number of jokes is small), Tf-Idf gives many tokens the same weight."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Retrieving relevant jokes based on TF-IDF embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (0, 498)\t0.8393828598192675\n",
            "  (0, 258)\t0.5435406283265565\n",
            "['teacher' 'joke']\n",
            "0.3822465320495546 \"Where do math teachers go on vacation?\" \"Times Square.\"\n",
            "0.30800626713050494 \"When does a joke become a dad joke? When it becomes apparent.\"\n",
            "0.21290086669346053 \"Why don't eggs tell jokes? They'd crack each other up.\"\n",
            "0.1953847899498888 \"I was going to tell a time-traveling joke, but you guys didn't like it.\"\n",
            "0.18610540215205892 \"I have a joke about chemistry, but I don't think it will get a reaction.\"\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "query = \"a cat and a dog\"\n",
        "query = \"I need a joke about students and teachers\"\n",
        "\n",
        "# transform the query: apply the same preprocessing and vectorization as for the jokes\n",
        "tfidf_query = tfidf_vectorizer.transform([preprocessor.preprocess_text(query)])\n",
        "\n",
        "# tf-idf scores for the query\n",
        "print(tfidf_query)\n",
        "# print the keywords\n",
        "print(tfidf_vectorizer.get_feature_names_out()[tfidf_query.nonzero()[1]])\n",
        "\n",
        "# find the most similar joke\n",
        "similarities = cosine_similarity(tfidf_query, tfidf_jokes)  # the similarities between the query and the jokes\n",
        "          # cosine_similarity returns a matrix with the similarities between each pair of jokes, we need only the first row\n",
        "\n",
        "# print the 5 most relevant (similar) jokes\n",
        "# sort the similarities\n",
        "sorted_similarities = similarities.argsort()[0][::-1]  # argsort returns the indices that would sort the array\n",
        "# print the most similar jokes\n",
        "for i in range(5):\n",
        "    print(similarities[0][sorted_similarities[i]] , jokes[sorted_similarities[i]])    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Word2vec embedding\n",
        "\n",
        "Word2Vec is a word embedding technique that represents words as dense vectors in a continuous vector space. Developed by researchers at Google, Word2Vec is trained on large text corpora to learn distributed representations of words based on their context. The key idea behind Word2Vec is that words with similar meanings tend to occur in similar contexts, and therefore should have similar vector representations. There are two main architectures for training Word2Vec embeddings: Continuous Bag of Words (CBOW) and Skip-gram.\n",
        "\n",
        "In the CBOW architecture, the model predicts the target word based on its context, which consists of surrounding words in a fixed window. The input to the model is the context words, and the output is the target word. Conversely, in the Skip-gram architecture, the model predicts surrounding context words given a target word. Both architectures use shallow neural networks with a single hidden layer to learn word embeddings.\n",
        "\n",
        "Word2Vec embeddings capture semantic relationships between words, enabling mathematical operations such as vector addition and subtraction to capture analogies and relationships between words. For example, the vector representation of \"king\" minus \"man\" plus \"woman\" might result in a vector close to the vector representation of \"queen.\" This property makes Word2Vec embeddings useful for various NLP tasks, including sentiment analysis, machine translation, and named entity recognition. Moreover, because Word2Vec embeddings capture semantic information in a dense vector space, they often outperform traditional sparse representations like one-hot encoding in terms of efficiency and effectiveness for downstream NLP tasks.\n",
        "\n",
        "We will use the `gensim` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Availanle pretrained embeddings: dict_keys(['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis'])\n",
            "Info about glove-twitter-25: {'num_records': 1193514, 'file_size': 109885004, 'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-25/__init__.py', 'license': 'http://opendatacommons.org/licenses/pddl/', 'parameters': {'dimension': 25}, 'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).', 'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-25.txt`.', 'read_more': ['https://nlp.stanford.edu/projects/glove/', 'https://nlp.stanford.edu/pubs/glove.pdf'], 'checksum': '50db0211d7e7a2dcd362c6b774762793', 'file_name': 'glove-twitter-25.gz', 'parts': 1}\n",
            "{\n",
            "  \"num_records\": 3000000,\n",
            "  \"file_size\": 1743563840,\n",
            "  \"base_dataset\": \"Google News (about 100 billion words)\",\n",
            "  \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-google-news-300/__init__.py\",\n",
            "  \"license\": \"not found\",\n",
            "  \"parameters\": {\n",
            "    \"dimension\": 300\n",
            "  },\n",
            "  \"description\": \"Pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in 'Distributed Representations of Words and Phrases and their Compositionality' (https://code.google.com/archive/p/word2vec/).\",\n",
            "  \"read_more\": [\n",
            "    \"https://code.google.com/archive/p/word2vec/\",\n",
            "    \"https://arxiv.org/abs/1301.3781\",\n",
            "    \"https://arxiv.org/abs/1310.4546\",\n",
            "    \"https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F189726%2Frvecs.pdf\"\n",
            "  ],\n",
            "  \"checksum\": \"a5e5354d40acb95f9ec66d5977d140ef\",\n",
            "  \"file_name\": \"word2vec-google-news-300.gz\",\n",
            "  \"parts\": 1\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# import gensim.downloader as api\n",
        "\n",
        "# api.info() returns information about available models\n",
        "\n",
        "# Print information about available models\n",
        "   \n",
        "# list the corpora and models available in gensim-data\n",
        "print(\"Availanle pretrained embeddings:\", api.info()['models'].keys())\n",
        "print(\"Info about glove-twitter-25:\", api.info()['models'][\"glove-twitter-25\"])\n",
        "\n",
        "# pretty print json with the information about the glove-twitter-25 model\n",
        "import json\n",
        "print(json.dumps(api.info()['models'][\"word2vec-google-news-300\"], indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding for 'apple': [-0.06445312 -0.16015625 -0.01208496  0.13476562 -0.22949219  0.16210938\n",
            "  0.3046875  -0.1796875  -0.12109375  0.25390625 -0.01428223 -0.06396484\n",
            " -0.08056641 -0.05688477 -0.19628906  0.2890625  -0.05151367  0.14257812\n",
            " -0.10498047 -0.04736328 -0.34765625  0.35742188  0.265625    0.00188446\n",
            " -0.01586914  0.00195312 -0.35546875  0.22167969  0.05761719  0.15917969\n",
            "  0.08691406 -0.0267334  -0.04785156  0.23925781 -0.05981445  0.0378418\n",
            "  0.17382812 -0.41796875  0.2890625   0.32617188  0.02429199 -0.01647949\n",
            " -0.06494141 -0.08886719  0.07666016 -0.15136719  0.05249023 -0.04199219\n",
            " -0.05419922  0.00108337 -0.20117188  0.12304688  0.09228516  0.10449219\n",
            " -0.00408936 -0.04199219  0.01409912 -0.02111816 -0.13476562 -0.24316406\n",
            "  0.16015625 -0.06689453 -0.08984375 -0.07177734 -0.00595093 -0.00482178\n",
            " -0.00089264 -0.30664062 -0.0625      0.07958984 -0.00909424 -0.04492188\n",
            "  0.09960938 -0.33398438 -0.3984375   0.05541992 -0.06689453 -0.04467773\n",
            "  0.11767578 -0.13964844 -0.26367188  0.17480469 -0.17382812 -0.40625\n",
            " -0.06738281 -0.07617188  0.09423828  0.20996094 -0.16308594 -0.08691406\n",
            " -0.0534668  -0.10351562 -0.07617188 -0.11083984 -0.03515625 -0.14941406\n",
            "  0.0378418   0.38671875  0.14160156 -0.2890625  -0.16894531 -0.140625\n",
            " -0.04174805  0.22753906  0.24023438 -0.01599121 -0.06787109  0.21875\n",
            " -0.42382812 -0.5625     -0.49414062 -0.3359375   0.13378906  0.01141357\n",
            "  0.13671875  0.0324707   0.06835938 -0.27539062 -0.15917969  0.00121307\n",
            "  0.01208496 -0.0039978   0.00442505 -0.04541016  0.08642578  0.09960938\n",
            " -0.04296875 -0.11328125  0.13867188  0.41796875 -0.28320312 -0.07373047\n",
            " -0.11425781  0.08691406 -0.02148438  0.328125   -0.07373047 -0.01348877\n",
            "  0.17773438 -0.02624512  0.13378906 -0.11132812 -0.12792969 -0.12792969\n",
            "  0.18945312 -0.13867188  0.29882812 -0.07714844 -0.37695312 -0.10351562\n",
            "  0.16992188 -0.10742188 -0.29882812  0.00866699 -0.27734375 -0.20996094\n",
            " -0.1796875  -0.19628906 -0.22167969  0.08886719 -0.27734375 -0.13964844\n",
            "  0.15917969  0.03637695  0.03320312 -0.08105469  0.25390625 -0.08691406\n",
            " -0.21289062 -0.18945312 -0.22363281  0.06542969 -0.16601562  0.08837891\n",
            " -0.359375   -0.09863281  0.35546875 -0.00741577  0.19042969  0.16992188\n",
            " -0.06005859 -0.20605469  0.08105469  0.12988281 -0.01135254  0.33203125\n",
            " -0.08691406  0.27539062 -0.03271484  0.12011719 -0.0625      0.1953125\n",
            " -0.10986328 -0.11767578  0.20996094  0.19921875  0.02954102 -0.16015625\n",
            "  0.00276184 -0.01367188  0.03442383 -0.19335938  0.00352478 -0.06542969\n",
            " -0.05566406  0.09423828  0.29296875  0.04052734 -0.09326172 -0.10107422\n",
            " -0.27539062  0.04394531 -0.07275391  0.13867188  0.02380371  0.13085938\n",
            "  0.00236511 -0.2265625   0.34765625  0.13574219  0.05224609  0.18164062\n",
            "  0.0402832   0.23730469 -0.16992188  0.10058594  0.03833008  0.10839844\n",
            " -0.05615234 -0.00946045  0.14550781 -0.30078125 -0.32226562  0.18847656\n",
            " -0.40234375 -0.3125     -0.08007812 -0.26757812  0.16699219  0.07324219\n",
            "  0.06347656  0.06591797  0.17285156 -0.17773438  0.00276184 -0.05761719\n",
            " -0.2265625  -0.19628906  0.09667969  0.13769531 -0.49414062 -0.27929688\n",
            "  0.12304688 -0.30078125  0.01293945 -0.1875     -0.20898438 -0.1796875\n",
            " -0.16015625 -0.03295898  0.00976562  0.25390625 -0.25195312  0.00210571\n",
            "  0.04296875  0.01184082 -0.20605469  0.24804688 -0.203125   -0.17773438\n",
            "  0.07275391  0.04541016  0.21679688 -0.2109375   0.14550781 -0.16210938\n",
            "  0.20410156 -0.19628906 -0.35742188  0.35742188 -0.11962891  0.35742188\n",
            "  0.10351562  0.07080078 -0.24707031 -0.10449219 -0.19238281  0.1484375\n",
            "  0.00057983  0.296875   -0.12695312 -0.03979492  0.13183594 -0.16601562\n",
            "  0.125       0.05126953 -0.14941406  0.13671875 -0.02075195  0.34375   ]\n",
            "Similar words to 'apple': [('apples', 0.720359742641449), ('pear', 0.6450697183609009), ('fruit', 0.6410146355628967), ('berry', 0.6302295327186584), ('pears', 0.613396167755127), ('strawberry', 0.6058261394500732), ('peach', 0.6025872230529785), ('potato', 0.5960935354232788), ('grape', 0.5935863852500916), ('blueberry', 0.5866668224334717)]\n"
          ]
        }
      ],
      "source": [
        "# Download pre-trained Word2Vec model\n",
        "model = api.load(\"word2vec-google-news-300\")   # 1GB MB to download (and load)\n",
        "\n",
        "# Get embedding for a specific word\n",
        "embedding = model[\"apple\"]\n",
        "print(\"Embedding for 'apple':\", embedding)\n",
        "\n",
        "# Find similar words\n",
        "similar_words = model.most_similar(\"apple\")\n",
        "print(\"Similar words to 'apple':\", similar_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding for 'cat': [ 0.0123291   0.20410156 -0.28515625  0.21679688  0.11816406  0.08300781\n",
            "  0.04980469 -0.00952148  0.22070312 -0.12597656  0.08056641 -0.5859375\n",
            " -0.00445557 -0.296875   -0.01312256 -0.08349609  0.05053711  0.15136719\n",
            " -0.44921875 -0.0135498   0.21484375 -0.14746094  0.22460938 -0.125\n",
            " -0.09716797  0.24902344 -0.2890625   0.36523438  0.41210938 -0.0859375\n",
            " -0.07861328 -0.19726562 -0.09082031 -0.14160156 -0.10253906  0.13085938\n",
            " -0.00346375  0.07226562  0.04418945  0.34570312  0.07470703 -0.11230469\n",
            "  0.06738281  0.11230469  0.01977539 -0.12353516  0.20996094 -0.07226562\n",
            " -0.02783203  0.05541992 -0.33398438  0.08544922  0.34375     0.13964844\n",
            "  0.04931641 -0.13476562  0.16308594 -0.37304688  0.39648438  0.10693359\n",
            "  0.22167969  0.21289062 -0.08984375  0.20703125  0.08935547 -0.08251953\n",
            "  0.05957031  0.10205078 -0.19238281 -0.09082031  0.4921875   0.03955078\n",
            " -0.07080078 -0.0019989  -0.23046875  0.25585938  0.08984375 -0.10644531\n",
            "  0.00105286 -0.05883789  0.05102539 -0.0291748   0.19335938 -0.14160156\n",
            " -0.33398438  0.08154297 -0.27539062  0.10058594 -0.10449219 -0.12353516\n",
            " -0.140625    0.03491211 -0.11767578 -0.1796875  -0.21484375 -0.23828125\n",
            "  0.08447266 -0.07519531 -0.25976562 -0.21289062 -0.22363281 -0.09716797\n",
            "  0.11572266  0.15429688  0.07373047 -0.27539062  0.14257812 -0.0201416\n",
            "  0.10009766 -0.19042969 -0.09375     0.14160156  0.17089844  0.3125\n",
            " -0.16699219 -0.08691406 -0.05004883 -0.24902344 -0.20800781 -0.09423828\n",
            " -0.12255859 -0.09472656 -0.390625   -0.06640625 -0.31640625  0.10986328\n",
            " -0.00156403  0.04345703  0.15625    -0.18945312 -0.03491211  0.03393555\n",
            " -0.14453125  0.01611328 -0.14160156 -0.02392578  0.01501465  0.07568359\n",
            "  0.10742188  0.12695312  0.10693359 -0.01184082 -0.24023438  0.0291748\n",
            "  0.16210938  0.19921875 -0.28125     0.16699219 -0.11621094 -0.25585938\n",
            "  0.38671875 -0.06640625 -0.4609375  -0.06176758 -0.14453125 -0.11621094\n",
            "  0.05688477  0.03588867 -0.10693359  0.18847656 -0.16699219 -0.01794434\n",
            "  0.10986328 -0.12353516 -0.16308594 -0.14453125  0.12890625  0.11523438\n",
            "  0.13671875  0.05688477 -0.08105469 -0.06152344 -0.06689453  0.27929688\n",
            " -0.19628906  0.07226562  0.12304688 -0.20996094 -0.22070312  0.21386719\n",
            " -0.1484375  -0.05932617  0.05224609  0.06445312 -0.02636719  0.13183594\n",
            "  0.19433594  0.27148438  0.18652344  0.140625    0.06542969 -0.14453125\n",
            "  0.05029297  0.08837891  0.12255859  0.26757812  0.0534668  -0.32226562\n",
            " -0.20703125  0.18164062  0.04418945 -0.22167969 -0.13769531 -0.04174805\n",
            " -0.00286865  0.04077148  0.07275391 -0.08300781  0.08398438 -0.3359375\n",
            " -0.40039062  0.01757812 -0.18652344 -0.0480957  -0.19140625  0.10107422\n",
            "  0.09277344 -0.30664062 -0.19921875 -0.0168457   0.12207031  0.14648438\n",
            " -0.12890625 -0.23535156 -0.05371094 -0.06640625  0.06884766 -0.03637695\n",
            "  0.2109375  -0.06005859  0.19335938  0.05151367 -0.05322266  0.02893066\n",
            " -0.27539062  0.08447266  0.328125    0.01818848  0.01495361  0.04711914\n",
            "  0.37695312 -0.21875    -0.03393555  0.01116943  0.36914062  0.02160645\n",
            "  0.03466797  0.07275391  0.16015625 -0.16503906 -0.296875    0.15039062\n",
            " -0.29101562  0.13964844  0.00448608  0.171875   -0.21972656  0.09326172\n",
            " -0.19042969  0.01599121 -0.09228516  0.15722656 -0.14160156 -0.0534668\n",
            "  0.03613281  0.23632812 -0.15136719 -0.00689697 -0.27148438 -0.07128906\n",
            " -0.16503906  0.18457031 -0.08398438  0.18554688  0.11669922  0.02758789\n",
            " -0.04760742  0.17871094  0.06542969 -0.03540039  0.22949219  0.02697754\n",
            " -0.09765625  0.26953125  0.08349609 -0.13085938 -0.10107422 -0.00738525\n",
            "  0.07128906  0.14941406 -0.20605469  0.18066406 -0.15820312  0.05932617\n",
            "  0.28710938 -0.04663086  0.15136719  0.4921875  -0.27539062  0.05615234]\n",
            "Similar words to 'cat': [('cats', 0.8099379539489746), ('dog', 0.760945737361908), ('kitten', 0.7464984655380249), ('feline', 0.7326233983039856), ('beagle', 0.7150582671165466), ('puppy', 0.7075453996658325), ('pup', 0.6934291124343872), ('pet', 0.6891531348228455), ('felines', 0.6755931377410889), ('chihuahua', 0.6709762215614319)]\n"
          ]
        }
      ],
      "source": [
        "# Get embedding for a specific word\n",
        "embedding = model[\"cat\"]\n",
        "\n",
        "# Find similar words\n",
        "similar_words = model.most_similar(\"cat\")\n",
        "\n",
        "print(\"Embedding for 'cat':\", embedding)\n",
        "print(\"Similar words to 'cat':\", similar_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise king - man + woman = ?\n",
        "Test the capability of Word2Vec embeddings to compute analogies by completing the analogy \"king - man + woman = ?\" and observe the resulting word vector.\n",
        "\n",
        "Test also: Paris is to France as Rome is to ________?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code goes here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('king', 0.8449392318725586), ('queen', 0.7300518155097961), ('monarch', 0.645466148853302), ('princess', 0.6156251430511475), ('crown_prince', 0.5818676948547363), ('prince', 0.5777117609977722), ('kings', 0.5613664388656616), ('sultan', 0.5376776456832886), ('Queen_Consort', 0.5344247221946716), ('queens', 0.5289887189865112)]\n"
          ]
        }
      ],
      "source": [
        "# Solution\n",
        "# Experiment king - man + woman = ?\n",
        "\n",
        "# Get embedding for a specific word\n",
        "embedding = model[\"king\"] - model[\"man\"] + model[\"woman\"]\n",
        "# Find similar words\n",
        "similar_words = model.most_similar([embedding])\n",
        "print (similar_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Italy', 0.7115296125411987), ('Rome', 0.7092384696006775), ('France', 0.590425431728363), ('Sicily', 0.5600441694259644), ('Italians', 0.5599856376647949), ('Flaminio_Stadium', 0.5327231884002686), ('Bambino_Gesu_Hospital', 0.505158007144928), ('Italian', 0.4975103735923767), ('Spain', 0.4952991306781769), ('Antonio_Martino', 0.4828406870365143)]\n"
          ]
        }
      ],
      "source": [
        "# Get embedding for a specific word\n",
        "embedding = model[\"France\"] - model[\"Paris\"] + model[\"Rome\"]\n",
        "\n",
        "# Find similar words\n",
        "similar_words = model.most_similar([embedding])\n",
        "print (similar_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "def word2vec_vectorizer(words, model):\n",
        "    \"\"\"Returns the average word embedding for the words in the input list. \n",
        "    If a word is not in the model's vocabulary, it is ignored.\"\"\"\n",
        "    embedding = []\n",
        "    for word in words:\n",
        "        if word in model.key_to_index:\n",
        "            embedding.append(model[word])\n",
        "  \n",
        "    if embedding:\n",
        "        embedding = sum(embedding) / len(embedding)\n",
        "        return embedding\n",
        "    return np.zeros(model.vector_size)\n",
        "\n",
        "# word2vec_vectorizer([\"caffft\", \"dofffg\"], model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Generate embeddings for jokes\n",
        "\n",
        "# For each joke, its embedding is the average embedding of all words in the joke \n",
        "joke_embeddings = []\n",
        "for joke in preprocessed_jokes:\n",
        "    joke_embedding = word2vec_vectorizer(joke, model)\n",
        "    joke_embeddings.append(joke_embedding)  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Retrieve jokes based on query and Word2Vec embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6684581 \"Where do math teachers go on vacation?\" \"Times Square.\"\n",
            "0.5728766 \"Why did the math book look so sad? Because of all of its problems!\"\n",
            "0.5301609 \"I like telling Dad jokes. Sometimes he laughs!\"\n",
            "0.5207011 \"I have a joke about chemistry, but I don't think it will get a reaction.\"\n",
            "0.51812315 \"I was going to tell a time-traveling joke, but you guys didn't like it.\"\n"
          ]
        }
      ],
      "source": [
        "query = \"I need a joke about students and teachers\"\n",
        "\n",
        "# Generate embedding for the query\n",
        "query_embedding = word2vec_vectorizer(preprocessor.preprocess_text(query), model)\n",
        "\n",
        "# Calculate cosine similarity between query and jokes\n",
        "similarities = cosine_similarity([query_embedding], joke_embeddings)\n",
        "\n",
        "\n",
        "# print the 5 most similar jokes\n",
        "# sort the similarities\n",
        "sorted_similarities = similarities.argsort()[0][::-1]  # argsort returns the indices that would sort the array\n",
        "# print the most similar jokes\n",
        "for i in range(5):\n",
        "   # print(jokes[sorted_similarities[i]])\n",
        "    print(similarities[0][sorted_similarities[i]] , jokes[sorted_similarities[i]])    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Comparison\n",
        "Qualitatively compare the relevance of the jokes retrieved by using TF-IDF embedding and Word2Vec embeddings. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: snowman\n",
            "TF-IDF:\n",
            "0.478 \"What do you call it when a snowman throws a tantrum?\" \"A meltdown.\"\n",
            "0.000 \"If you see a crime happen at the Apple store, what does it make you?\" \"An iWitness.\n",
            "0.000 \"I have a joke about chemistry, but I don't think it will get a reaction.\"\n",
            "0.000 \"Why didn't the skeleton climb the mountain?\" \"It didn't have the guts.\"\n",
            "0.000 \"What time did the man go to the dentist? Tooth hurt-y.\"\n",
            "\n",
            "Word2Vec:\n",
            "0.566 \"What do you call it when a snowman throws a tantrum?\" \"A meltdown.\"\n",
            "0.535 \"How does a penguin build its house? Igloos it together.\"\n",
            "0.486 \"How much does it cost Santa to park his sleigh?\" \"Nothing, it's on the house.\"\n",
            "0.385 \"How do you get a good price on a sled?\" \"You have toboggan.\"\n",
            "0.380 \"How can you tell if a tree is a dogwood tree?\" \"By its bark.\"\n"
          ]
        }
      ],
      "source": [
        "# Aggregated code, comparing the two methods\n",
        "\n",
        "query = \"snowman\"\n",
        "\n",
        "# Preprocess the query\n",
        "preprocessed_query = preprocessor.preprocess_text(query)\n",
        "\n",
        "# Embed the query into the two spaces\n",
        "tfidf_query = tfidf_vectorizer.transform([preprocessed_query])\n",
        "word2vec_query = word2vec_vectorizer(preprocessed_query, model)\n",
        "\n",
        "# Compute the cosine similarities\n",
        "tfidf_similarities = cosine_similarity(tfidf_query, tfidf_jokes)\n",
        "word2vec_similarities = cosine_similarity([word2vec_query], joke_embeddings)\n",
        "\n",
        "\n",
        "# Get the top 5 most similar jokes for each method\n",
        "sorted_tfidf_similarities = tfidf_similarities.argsort()[0][::-1]\n",
        "sorted_word2vec_similarities = word2vec_similarities.argsort()[0][::-1]\n",
        "\n",
        "# Print the top 5 most similar jokes for each method\n",
        "print(\"Query:\", query)\n",
        "print(\"TF-IDF:\")\n",
        "for i in range(5):\n",
        "  print(\"{:.3f}\".format(tfidf_similarities[0][sorted_tfidf_similarities[i]]), jokes[sorted_tfidf_similarities[i]])\n",
        "\n",
        "print(\"\\nWord2Vec:\")\n",
        "for i in range(5):\n",
        "  print(\"{:.3f}\".format(word2vec_similarities[0][sorted_word2vec_similarities[i]]), jokes[sorted_word2vec_similarities[i]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Exercises\n",
        "1. Change the dataset to Grimm's fairy tales: https://www.kaggle.com/datasets/tschomacker/grimms-fairy-tales or another text dataset of your interest.\n",
        "2. Cluster the jokes, find the medoids, find keywords for each cluster.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Review Questions\n",
        "1. What does TF-IDF stand for, and what does it represent in natural language processing?\n",
        "2. Explain the intuition behind TF-IDF\n",
        "3. What are the advantages of using TF-IDF over simple word frequency for text representation?\n",
        "4. How are TF-IDF scores calculated for individual terms in a document?\n",
        "5. How does TF-IDF handle stopwords and rare terms in a document collection?\n",
        "6. Explain the term weighting scheme used in TF-IDF and how it affects the importance of terms in documents.\n",
        "7. What are some limitations or challenges associated with using TF-IDF?\n",
        "8. What is Word2Vec?\n",
        "9. Describe the Skip-gram architecture.\n",
        "10. Discuss the training objective of Word2Vec and how it learns to capture semantic similarities between words.\n",
        "11. What are some advantages of using Word2Vec embeddings over traditional one-hot encodings or bag-of-words representations?\n",
        "12. Explain the notion of cosine similarity in word embeddings.\n",
        "13. How can Word2Vec embeddings be evaluated and validated for their quality and effectiveness?\n",
        "14. Discuss the transfer learning capabilities of Word2Vec embeddings and their applications in downstream NLP tasks.\n",
        "15. Compare and contrast text representation techniques, such as bag-of-words, TF-IDF and word embeddings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Conclusion\n",
        "This notebook serves as a demonstration of a basic information retrieval system, illustrating the utilization of both TF-IDF and Word2Vec embeddings. While TF-IDF treats words in isolation, lacking contextual understanding, pre-trained Word2Vec embeddings capture semantic relationships by leveraging vast corpora through self-supervised learning. \n",
        "\n",
        "Beyond information retrieval, these embeddings find utility in a variety of tasks including text classification, topic detection, and beyond."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "cac635b2bbe20859b80c6b711019aad22d51ab596f6bdd91b0296498d6b3846b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
