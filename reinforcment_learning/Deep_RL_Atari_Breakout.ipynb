{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Initial steps"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install gymnasium\n","!pip install gym\n","!pip install pygame"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9682,"status":"ok","timestamp":1708913026941,"user":{"displayName":"Hanics Mih√°ly","userId":"17395844450453431260"},"user_tz":-60},"id":"S-_roV2eJqZY","outputId":"cf6bf77a-8f2e-4213-c82f-b1e93d33a6ab"},"outputs":[],"source":["#Download these libraries if you don't have them\n","import gymnasium as gym\n","import numpy as np\n","import pygame"]},{"cell_type":"markdown","metadata":{},"source":["I recommend **checking out the previous notebook** (Q-learning CartPole) to get a better understanding of the environment, the Q-function and how reinforcement learning models learn."]},{"cell_type":"markdown","metadata":{},"source":["We use the the OpenAI [Gym](https://github.com/openai/gym) this time, we need to downgrade to an older version too. As before, it provides easy and nice-to-use environments for reinforcement learning.<br>\n","\n","The game for this model:\n","\n","- `Atari Breakout`: Given many blocks positioned on the top of the screen, a paddle on the bottom, and an always moving ball. You control a paddle (moving it left and right) to bounce the ball off of it, when the ball hits a brick it breaks it and bounces off of it. The goal is to break all bricks, before the ball falls off the screen.\n","\n","A gif:\n","<div>\n","    <img src=\"https://miro.medium.com/v2/resize:fit:1760/1*XyIpmXXAjbXerDzmGQL1yA.gif\", width=\"400\">\n","</div>\n","\n","The model we use will be a Deep Q-Network model. It is based on the 2013 paper by DeepMind (Minh et al.) [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602).<br>\n","The code is taken from the Keras website [here](https://keras.io/examples/rl/deep_q_network_breakout/).\n"]},{"cell_type":"markdown","metadata":{},"source":["To get the environment (with states, etc.), we just need to call the `gym.make()` function with the name of the environment.\n","\n","If we use `render_mode='human'`, we can see the environment in a pop-up window, however this slows learning. We will only use it for demonstration of the final model.\n","\n","We can close the environment with the `env.close()` function."]},{"cell_type":"markdown","metadata":{},"source":["## Deep Q-Learning"]},{"cell_type":"markdown","metadata":{},"source":["Example + code taken from the Keras website: [Deep Q-Learning for Atari Breakout](https://keras.io/examples/rl/deep_q_network_breakout/)"]},{"cell_type":"markdown","metadata":{},"source":["**Atari Breakout**\n","\n","Environment: \n","- Image input: 210x160x3, but using the helper function it is turned into 4 consecutive 84x84 images\n","- 4 actions: 0: do nothing, 1: fire, 2: move right, 3: move left\n","- Reward: 1 for each brick broken\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["You'll need to install `baselines` (e.g., with `pip install git+https://github.com/openai/baselines.git` in the current environment) to use the Atari environment helper functions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install git+https://github.com/openai/baselines.git"]},{"cell_type":"code","execution_count":135,"metadata":{},"outputs":[],"source":["from baselines.common.atari_wrappers import make_atari, wrap_deepmind\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers"]},{"cell_type":"markdown","metadata":{},"source":["To use GPUs for TensorFlow on Windows, you may need to downgrade to TF 2.10, or use WSL2"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Gym: 0.21.0\n","Ale-py: 0.7.4\n","Pyglet: 2.0.12\n","Tensorflow: 2.15.0\n"]}],"source":["import gym, ale_py, pyglet\n","\n","print(\"Gym:\", gym.__version__), print(\"Ale-py:\", ale_py.__version__), print(\"Pyglet:\", pyglet.__version__),print(\"Tensorflow:\", tf.__version__);"]},{"cell_type":"markdown","metadata":{},"source":["I recommend using a separate Python environment for this. For Windows, I use Pyenv."]},{"cell_type":"markdown","metadata":{},"source":["Ale_py downgrade to 0.7.4 is needed to run the code, else *env.reset()* will not necessary work.\n","\n","For training the model, you can downgrade to just Gym 0.25.2, but for the display in the end, you'll need to use 0.21.0 or below"]},{"cell_type":"markdown","metadata":{},"source":["Configurations:"]},{"cell_type":"code","execution_count":134,"metadata":{},"outputs":[],"source":["seed = 42 #Reproducibility\n","gamma = 0.99 \n","epsilon = 1.0 #epsilon greedy parameter\n","epsilon_min = 0.1\n","epsilon_max = 1.0\n","epsilon_interval = (\n","    epsilon_max - epsilon_min\n",")  #Rate at which to reduce chance of random action being taken. In the beginning, it is 100% random, in the end, it is 10% random\n","batch_size = 32  # Size of batch taken from replay buffer\n","max_steps_per_episode = 10000\n","\n","# Use the Baseline Atari environment because of Deepmind helper functions\n","env = make_atari(\"BreakoutNoFrameskip-v4\")\n","# Warp the frames, grey scale, stake four frame and scale to smaller ratio\n","env = wrap_deepmind(env, frame_stack=True, scale=True)\n","env.seed(seed)\n","env.reset();\n"]},{"cell_type":"markdown","metadata":{},"source":["Epsilon now is reducing linearly from 1 to 0.1 over 1 million steps, then stays at 0.1. For the first 50000 steps, there is only exploring (see below code)"]},{"cell_type":"markdown","metadata":{},"source":["The DeepMind wrap helps to preprocess the images, and also stacks 4 frames together to give the model a sense of motion (for the ball)"]},{"cell_type":"code","execution_count":136,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From c:\\Users\\hanic\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n","\n"]}],"source":["num_actions = 4\n","\n","def create_q_model():\n","    # Network defined by the Deepmind paper\n","    inputs = layers.Input(shape=(84, 84, 4,)) #The DeepMind wrap creates 84x84 size frames\n","\n","    #Convolutions on the frames on the screen\n","    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n","    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n","    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n","\n","    layer4 = layers.Flatten()(layer3)\n","\n","    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n","    action = layers.Dense(num_actions, activation=\"linear\")(layer5)\n","\n","    return keras.Model(inputs=inputs, outputs=action)\n","\n","#Two models (they are trained together, but the target model is updated less often): a Q-function learning model and a target model.\n","#First model changes the Q-values.\n","model = create_q_model()\n","\n","#The second \"target\" model is for predicting future rewards.\n","#The weights of a target model are updated every 10000 steps, thus when the loss between the Q-values is calculated \n","# the target Q-value will be stable.\n","model_target = create_q_model()"]},{"cell_type":"markdown","metadata":{},"source":["The environment is inputted as a 84x84 image, but we input 4 consecutive frames as a single state (so the input shape is 84x84x4). This was the idea in the DeepMind Atari paper, and it helps the model understand the movement of the ball from frame to frame.\n","\n","We use 3 convolutional layers, then a flatten layer, then 2 dense layers for decision making, last one being the output layer with 4 neurons (one for each action)."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 18ms/step\n","...\n","1/1 [==============================] - 0s 73ms/step\n","1/1 [==============================] - 0s 74ms/step\n","1/1 [==============================] - 0s 104ms/step\n","1/1 [==============================] - 0s 81ms/step\n","\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[2], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(text)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m a \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["#Deepmind used RMSProp, but the Adam optimizer came out 1 year later and improves training time\n","optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n","\n","# Experience replay buffers\n","action_history = []\n","state_history = []\n","state_next_history = []\n","rewards_history = []\n","done_history = []\n","episode_reward_history = []\n","running_reward = 0\n","episode_count = 0\n","frame_count = 0\n","\n","# Number of frames to take random action and observe output\n","epsilon_random_frames = 50000\n","\n","# Number of frames for exploration\n","epsilon_greedy_frames = 1000000.0\n","\n","# Maximum replay length\n","# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n","max_memory_length = 100000\n","\n","# Train the model after 4 actions\n","update_after_actions = 4\n","\n","# How often to update the target network\n","update_target_network = 10000\n","\n","# Using huber loss for stability\n","loss_function = keras.losses.Huber()\n","\n","while True:  #Should run until solved (see below: 40 score), but we can stop it manually.\n","    state = np.array(env.reset())\n","    episode_reward = 0\n","\n","    for timestep in range(1, max_steps_per_episode):\n","        # env.render(); Adding this line would show the attempts\n","        frame_count += 1\n","        if frame_count % 10000 == 0:\n","            model.save_weights('model_weights.h5') #Just in case something happens\n","\n","        # Epsilon-greedy exploration, as before\n","        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n","            #Exploration: take random action\n","            action = np.random.choice(num_actions)\n","        else:\n","            #Exploitation: take the best action\n","            state_tensor = tf.convert_to_tensor(state)\n","            state_tensor = tf.expand_dims(state_tensor, 0)\n","            action_probs = model(state_tensor, training=False)\n","\n","            action = tf.argmax(action_probs[0]).numpy()\n","\n","        #Epsilon value decayed, unless it is at minimum. These lines of code can be put somewhere else too.\n","        if epsilon > epsilon_min: #I changed this part in the code, as it was a bit weirdly written.\n","            epsilon -= epsilon_interval / epsilon_greedy_frames\n","        \n","        #We make the action, take the outputs\n","        state_next, reward, done, _ = env.step(action)\n","        state_next = np.array(state_next)\n","\n","        episode_reward += reward\n","\n","        # Save actions and states in replay buffer\n","        action_history.append(action)\n","        state_history.append(state)\n","        state_next_history.append(state_next)\n","        done_history.append(done)\n","        rewards_history.append(reward)\n","        state = state_next\n","\n","        # Update every fourth frame and once batch size is over 32\n","        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n","\n","            # Get indices of samples for replay buffers\n","            indices = np.random.choice(range(len(done_history)), size=batch_size)\n","\n","            # Using list comprehension to sample from replay buffer\n","            state_sample = np.array([state_history[i] for i in indices])\n","            state_next_sample = np.array([state_next_history[i] for i in indices])\n","            rewards_sample = [rewards_history[i] for i in indices]\n","            action_sample = [action_history[i] for i in indices]\n","            done_sample = tf.convert_to_tensor(\n","                [float(done_history[i]) for i in indices]\n","            )\n","\n","            # Build the updated Q-values for the sampled future states\n","            # Use the target model for stability\n","            future_rewards = model_target.predict(state_next_sample)\n","            # Q value = reward + discount factor * expected future reward\n","            updated_q_values = rewards_sample + gamma * tf.reduce_max(\n","                future_rewards, axis=1\n","            )\n","\n","            # If final frame set the last value to -1 (done)\n","            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n","\n","            #One-hot encoded mask, we only calculate loss on the updated Q-values\n","            masks = tf.one_hot(action_sample, num_actions)\n","\n","            with tf.GradientTape() as tape:\n","                # Train the model on the states and updated Q-values\n","                q_values = model(state_sample)\n","\n","                # Apply the masks to the Q-values to get the Q-value for action taken\n","                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n","                # Calculate loss between new Q-value and old Q-value\n","                loss = loss_function(updated_q_values, q_action)\n","\n","            # Backpropagation\n","            grads = tape.gradient(loss, model.trainable_variables)\n","            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","\n","        if frame_count % update_target_network == 0:\n","            # update the the target network with new weights\n","            model_target.set_weights(model.get_weights())\n","            # Log details\n","            template = \"running reward: {:.2f} at episode {}, frame count {}\"\n","            print(template.format(running_reward, episode_count, frame_count))\n","\n","        # Limit the state and reward history\n","        if len(rewards_history) > max_memory_length:\n","            del rewards_history[:1]\n","            del state_history[:1]\n","            del state_next_history[:1]\n","            del action_history[:1]\n","            del done_history[:1]\n","\n","        if done:\n","            break\n","\n","    # Update running reward to check condition for solving\n","    episode_reward_history.append(episode_reward)\n","    if len(episode_reward_history) > 100:\n","        del episode_reward_history[:1]\n","    running_reward = np.mean(episode_reward_history)\n","\n","    episode_count += 1\n","\n","    if running_reward > 40:  # Condition to consider the task solved\n","        print(\"Solved at episode {}!\".format(episode_count))\n","        break"]},{"cell_type":"markdown","metadata":{},"source":["Stopped running after ~24 hours (no GPU), the model is already fairly good but has to learn to perfect the game.\n","\n","Used `model.save_weights('model_weights_last.h5')` to save the last model's weights, but we could have also just used the already saved `model_weights.h5` file."]},{"cell_type":"markdown","metadata":{},"source":["Let's load the model and try it:"]},{"cell_type":"code","execution_count":137,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\hanic\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\gym\\envs\\atari\\environment.py:267: UserWarning: \u001b[33mWARN: We strongly suggest supplying `render_mode` when constructing your environment, e.g., gym.make(ID, render_mode='human'). Using `render_mode` provides access to proper scaling, audio support, and proper framerates.\u001b[0m\n","  logger.warn(\n"]},{"data":{"text/plain":["True"]},"execution_count":137,"metadata":{},"output_type":"execute_result"}],"source":["model.load_weights('model_weights_last.h5')\n","\n","# Now you can use the render method to visualize the game\n","env.render('human')"]},{"cell_type":"markdown","metadata":{},"source":["A bit of testing:"]},{"cell_type":"code","execution_count":138,"metadata":{},"outputs":[],"source":["num_episodes = 10  # Number of games to play\n","max_frames_test = 2500\n","\n","for i in range(num_episodes):\n","    state = np.array(env.reset())\n","    done = False\n","    frames = 0\n","    while (not done) and (frames < max_frames_test):\n","        env.render(\"human\") #Could also just not render\n","        state_tensor = tf.convert_to_tensor(state)\n","        state_tensor = tf.expand_dims(state_tensor, 0)\n","        action_probs = model(state_tensor, training=False)\n","        action = tf.argmax(action_probs[0]).numpy()#We only exploit in testing\n","        state, reward, done, _ = env.step(action)\n","        frames += 1\n","\n","env.close()"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOW4PYdLnYIt1DSTd/P4XsR","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
